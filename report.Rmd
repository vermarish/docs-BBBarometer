---
title: "Big Brother Barometer"
author: "Rishabh Verma"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    toc: true
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally) # for ggpairs scatterplot matrix
library(ggpubr) # for multiple ggplots in one figure
library(tidyverse)
library(glmnet)

## Filepath on my desktop
# setwd("E:\\Desktop\\docs-BBBarometer")

## Filepath on my laptop
setwd("C:\\Users\\risha\\OneDrive\\Desktop\\docs-BBBarometer")
```

## Introduction

My phone has a gyroscope and a barometer, which respectively measure rotational velocity and air pressure. This paper centers around the phenomenon that when I touch the screen, the barometer registers a spike in air pressure, likely because the force of my finger causes a volume deflection inside the phone. The gyroscope is susceptible to recording tremors in the hand holding the phone, but if the barometer can indicate WHEN touch input occurs, the gyroscope can indicate WHERE on the screen the touch input occurs. This could be applied to create a malicious background process which can read a user's PIN code with access only to barometer and gyroscope sensor data. This report is a work in progress.



```{r include=FALSE}
#tidbits <- read_csv("data/experiment_1.csv")

# TODO just use the line above
#tidbits <- read_csv("data/trial.csv")
tidbits <- read_csv("data/sporadic.csv")
```

## Data Cleaning


```{r}
# Clean tidbits and create tibble `data`
source("cleaning_and_plotting.R")
```


## 1: Finding touch occurrence from pressure data

Now to detect touch occurrence. I could do something dumb and threshold the pressure data, or I could do something clever and train a model. Perhaps I keep a buffer of the last four pressure readings, or simply the last four changes in pressure reading. This will be tuple of 4 predictors. To form a training dataset, I need responses; perhaps the four-tuple nearest in time

                 |                              |     
. . . . . x x x x|. . .        . . . . . . . . .|x x x x . .
                 |                              |     
nearest in time before            nearest in time after
   (unviable)                            (viable)

can be categorized as 1, and all other four-tuples (excepting maybe the neighboring ones) can be categorized as 0.

After training a binary classifier (logit distribution), we will then threshold on the logit probability. 

### 1a: Examine pressure data

```{r}
scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=0)
scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=1, stem=TRUE)
scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=2, stem=TRUE)
```

It looks like each pressure spike is registered on about two points. Let's use a window of four derivative values, which requires computation on a window of size five. Around each touch point, I will record two subsequent windows as label "one".

### 1b: Label pressure data with touch input

Let's use the first 60\% of the data as training data, and the last 40\% as testing data. With 161 data points in experiment 1, this is 101 data points in training and 69 data points in testing.

```{r}
prop = 0.6  # proportion of training data


## Portion data into train and test
touch_data <- data %>% 
  filter(type=="touch") %>%
  arrange(time)

cutoff <- touch_data$time[as.integer(nrow(touch_data)*prop)]

data <- data %>%
  mutate(set = sapply(time, function(t) {
    if (t < cutoff) "training" else "testing"
  }))

train <- data %>%
  filter(time < cutoff) %>%
  filter(type %in% c("pressure", "touch")) %>%
  arrange(time)

test <- data %>%
  filter(time > cutoff) %>%
  filter(type %in% c("pressure", "touch")) %>%
  arrange(time)
```

Next, we build a dataframe containing snapshots of pressure data labeled with 1 or 0

```{r}
source("phase_one_functions.R")
```

### 1c: Predict touch input from pressure data

Let's just try a logistic regression. I'm concerned this isn't really the best choice of model because we don't have a close-to-even split and the noise may not be gaussian. This also does not leverage the fact that we are dealing with a time series, but let's just try it out.


```{r}
# this is just pressure data with a p-column
touch_confidence <- train_pressure_model(train, test, width=4)

# Using that p-column, we can try a bunch of different thresholds
performance_by_threshold <- evaluate_thresholds(data, touch_confidence)

# This threshold did pretty well
threshold <- performance_by_threshold %>%
  slice_max(F1) %>%
  slice(1) %>%
  pull(threshold)
# So let's use that threshold.

# this includes touch_predict and touch
thresholded <- threshold_pressure(touch_confidence, threshold=threshold) %>%
  bind_rows(data %>% filter(type=="touch"))

touch_predicts <- thresholded %>% select(-set) %>% filter(type=="touch_predict")

a=0.8
w=0.1

scatterplot_pressure_model(thresholded, touch_confidence,
                           a=a, b=a+w,
                           threshold=threshold)

scatterplot_pressure_model(bind_rows(data, touch_predicts), touch_confidence,
                           a=a, b=a+w,
                           threshold=threshold,
                           sensor="gyroscope", col="one")
```

In the top figure, the red vertical lines are physical taps. The grey signal is pressure data.

In the bottom figure, the red signal is the output of the "pressureâ†’touch" model. When this signal crosses a threshold, the model predicts a touch, displayed as a blue dashed vertical line.

<!-- Wow! That looks really good. I really hate proceeding forward without objective measures of everything I do. I really bet our model could be improved using lasso regression, and that would require hyperparameter tuning which is unreliable without a measurable error function, but "time to market" is of more importance. -->



### 1d: Evaluate model

#### The metrics

So we've got real touches (red lines) and predicted touches (dashed blue lines). 

* When a red line is accompanied by a dashed blue line, that's a true positive ($TP$).

* When a red line is not accompanied by a dashed blue line, that's a false negative ($FN$).

* When a dashed blue line is not near any red line, that's a false positive ($FP$).

* In this model, there is no meaningful notion of a true negative; there is nothing to count.

So we've got three quantities we can work with, and three meaningful metrics. 

What proportion of recorded events are actually real touches? This is the positive predictive value (PPV), equal to $\dfrac{TP}{TP+FP}$.

What proportion of real touches is the model able to capture? This is the true positive rate (TPR), equal to $\dfrac{TP}{TP+FN}$

Using a signal processing analogy, PPV describes how much of the measurement is noise, and TPR describes how much of the signal the measurement captures. A model that is too sensitive may have excellent TPR, but poor PPV as it keeps falsely triggering without a touch input. A model that is too strict may have excellent PPV, but poor TPR as it misses a lot of the signal.

Let's compare PPV and TPR at a bunch of different thresholds.

# TODO cd .. we were right here trying to get rid of the dependency on cutoff in line 234. Then we shall make this chunk into a function. Then we shall work on the writeup. Flow must be like: "Present data, build model, find threshold from training data, show model in signal form with this threshold (without further elaboration), THEN talk about thresholds -- show model in performance-summary form, talk about F-statistics"
  


```{r include=FALSE}
threshold_line <- geom_vline(xintercept=threshold, color="#D3C4BC", linetype=2)


plot1 <- performance_by_threshold %>%
  pivot_longer(cols = c(PPV, TPR), names_to = "metric") %>%
  ggplot(aes(x=threshold, y=value, color=metric)) +
  geom_line(size=1) +
  geom_point() +
  scale_y_continuous(limits=c(0,1)) +
  scale_x_reverse(limits=function(xlims) { c(1, xlims[2]) }) +
  scale_color_manual(breaks = c("PPV", "TPR"), 
                     values = c("#F8766D", "#619CFF"),
                     labels = c("PPV (reliability, precision)", 
                                "TPR (sensitivity, recall)")) +
  ggtitle("Precision and recall by decreasing threshold") +
  threshold_line


plot2 <- ggplot() +
  geom_col(data=performance_by_threshold %>%
             pivot_longer(cols=c(TP,FN), names_to="classification", values_to="count"),
           mapping=aes(x=threshold, y=count, fill=classification)) +
  scale_fill_manual(values=c("TP"="#1D8909",    #00BB44
                             "FN"="#CCDACA",    #A0CCA9
                             "FP"="#D01C8B")) + #EE4444  
  geom_col(data=performance_by_threshold,
           mapping=aes(x=threshold, y=-FP),
           fill="#D01C8B") +
  scale_x_reverse() +
  ggtitle("Classifications by decreasing threshold") +
  threshold_line

  
plot2b <- plot2 +
  geom_line(data=performance_by_threshold %>%
              pivot_longer(cols=c(F1, Fhalf, F2), names_to="metric", values_to="F") %>%
              mutate(metric = factor(metric, levels=c("Fhalf", "F1", "F2"))),
            mapping=aes(x=threshold, y=F*(TP+FN), linetype=metric, color=metric, size=metric)) +
  scale_linetype_manual(values=c("F2"=2, "F1"=1, "Fhalf"=3)) +
  scale_color_manual(values=c("F2"="#DB3E00", "F1"="#E7AB0B", "Fhalf"="#82BDFE")) +
  scale_size_manual(values=c("F2"=1, "F1"=1, "Fhalf"=1.3)) +
  ggtitle("Classifications by decreasing threshold",
          subtitle="with three precision-recall metrics")


plot3 <- performance_by_threshold %>%
  filter(PPV > 0) %>%
  pivot_longer(cols=c(F1, Fhalf, F2), names_to="Fscore", values_to="F") %>%
  mutate(Fscore = factor(Fscore, levels=c("F2","F1","Fhalf"))) %>%  # order, for facet_grid
  ggplot(aes(x=threshold, y=TPR, fill=F/max(F),
             # dummy aesthetics, for tooltip
             TP=TP, FP=FP)) +
  geom_col() +
  scale_x_reverse(limits=function(xlims) { c(1, xlims[2]) }) +
  scale_fill_continuous(type="viridis") +
  facet_grid(Fscore ~ .) +
  ggtitle("Classifications colored by three precision-recall metrics") +
  labs(y="TPR (sensitivity, recall)",
       fill="F (scaled to 1)") +
  threshold_line
```


> Let's see how threshold affects precision and recall.

From left to right, the threshold lowers and the model becomes more sensitive (Fig 1). As the recall (blue) approaches 1, the precision of positive predictions (red) decreases.
```{r}
plot1
```

> But proportions are too abstract; let's count the actual classifications. 

In Figure 2, the pale green bar represents the positives that need to be captured. From left to right, the threshold lowers and the model is able to capture true positives. As the threshold lowers too far, though, the model captures more and more false positives.

```{r}
plot2
```
 
> I understand "green good, red bad", but how about a measurable accuracy metric?

I thought about using the unit norm or the geometric mean of precision and recall, but I decided on F-score because it's [well-documented](https://en.wikipedia.org/wiki/F-score). I can weight it so that either precision ($F_{0.5}$) or recall ($F_2$) is more important. Let's throw that on the previous plot.
 
```{r}
plot2b
```

> Which metric is better?

You can clearly see that Fhalf is biased to the left, where precision is indeed higher, but it's difficult to see where each metric reaches its maximum. Let's take the green plot and color-code by each F-statistic. The coloring in Fig 3 is scaled so that the "best" threshold as measured by each metric is similarly bright.

```{r}
plot3
```


# TODO hmmmm.... 
<!--Now that I've written these plots up, as pretty as plot 3 is, I think it would be better to simply examine a lineplot with these metrics all scaled to 1, with the peaks annotated.-->

```{r}
plot4 <- performance_by_threshold %>%
  filter(PPV > 0) %>%
  pivot_longer(cols=c(F1, Fhalf, F2), names_to="Fscore", values_to="F") %>%
  mutate(Fscore = factor(Fscore, levels=c("F2","F1","Fhalf"))) %>%  # order, for legend
  ggplot(aes(x=threshold, y=F, color=Fscore)) +
  geom_line() # TODO cd .. we stuck here
  
plot4
```

1. I should try models with and without zero-intercepts.
2. I should try models with and without some accelerometer data.
3. I should double check whether logistic regression even makes sense, or if all linear models are identical.
4. I should figure out a metric to compare model performance in testing vs training to avoid overfitting. Maybe peak PPV*TPR in either?
5. Why are these PPV curves so weird?? 

* PPV = TP/(TP+FP). The denominator is the number of positives, and the numerator is the proportion of positives. It's actually pretty close to constant across all thresholds, and not that weird. This suggests that there isn't a ton of variance left to explain.

5a. Why does it suggest there isn't a ton of variance left to explain?

* Suppose there was unexplained variance. A strict model has high PPV. A looser model has proportionally lower PPV because the "less obvious" touches are difficult to distinguish from noise. 

6. Why are these TPR curves so weird??

* TPR = TP/(TP+FN). The denominator is a constant, equal to the number of "touch" datapoints in the training data. Thus, the plot of TPR~threshold is proportional to TP~threshold. The reason TP dips below full is because the concept of thresholding a signal makes no sense when that threshold is near the minimum value of the signal. For example, `r touch_confidence %>% ggplot(aes(x=time, y=p)) + geom_line() + geom_hline(yintercept=0.2, colour="grey", linetype=2)`  

* It might make sense to only use the threshold where TPR is non-increasing.

7. Once I have a model that's good, is it going to do well on an independent data source?


Well, at least I can typically do better than 80% PPV and 80% TPR.

### 1e: Tune model parameters

### 1f: Improve model (TODO)


## Phase 2: Mining features for direction of turn











Let's assume our touch input is good. Let's mine the subsequent gyroscope data for features. 

```{r}

model <- data %>%
  bind_rows(thresholded_testing %>%
              filter(type=="touch_predict") %>%
              select(-p)) %>%
  bind_rows(thresholded_training %>%
              filter(type=="touch_predict") %>%
              select(-p))

model %>% group_by(type) %>% count()

a=0.85
scatterplot(model, "gyroscope", "one", start=a, end=a+0.05)
```

Okay next phase! Given touch input (or a proxy for its occurrence), can I identify a relationship between gyroscope data and the location of the touch input?   

Which features do I mine? 

Perhaps the next 10 datapoints.
Perhaps their slopes.
Perhaps I subsample the next 10 datapoints.
Perhaps the point of greatest absolute value in the neighborhood.
  What is a neighborhood? Is it simply 10 points before and 10 points after? Or maybe I can be clever with it around the zeros. Suppose my pressure data points me to a positive accelerometer peak. I could consider everything between the surrounding zeros as the peak. I could use the width of this peak as a statistical measure.
  
This peak will have n points. How do I mine this peak?????

I really need some sort of book on statistical methods for time series

```{r}
# In: signal should be a tibble with two columns: time and data (pressure data)
#     times should be a vector of doubles.
# Ou: list of waveforms. (each waveform is itself represented as a list)
pull_waveforms <- function(signal, times) {
  features <- list()
  
  for (time in times) {
    center <- which(signal$time > time)[1]
    polarity <- sign(signal$data[center])
    left <- center
    right <- center
    
    while (left > 1 && sign(signal$data[left - 1]) == polarity) {
      left <- left - 1
    }
    while (right < nrow(signal) && sign(signal$data[right + 1]) == polarity) {
      right <- right + 1
    }
    feature <- signal$data[left:right]
    features[[length(features)+1]] = feature
  }
  return(features)
}


# In: TIME-SORTED data tibble with ONLY timestamped gyroscope entries
# Ou: feature tibble containing each wave
mine_features <- function(gyro, times) {
  # Mining features with pull_waveforms works better when delayed by 7-10 samples, to get closer to the center of a peak.
  delay <- 10
  period <- gyro$time %>% diff %>% mean
  times <- times + delay*period
  
  gyro_one <- pull_waveforms(signal=gyro %>% select(time=time, data=one),
                             times=times)
  
  gyro_two <- pull_waveforms(signal=gyro %>% select(time=time, data=two),
                             times=times)
  gyro_three <- pull_waveforms(signal=gyro %>% select(time=time, data=three),
                             times=times)
  
  # mine the extremum of each waveform as a feature
  extremum_one <- gyro_one %>% sapply(function(waveform) {sign(waveform[[1]][1]) * max(abs(waveform[[1]]))})
  extremum_two <- gyro_two %>% sapply(function(waveform) {sign(waveform[[1]][1]) * max(abs(waveform[[1]]))})
  extremum_three <- gyro_three %>% sapply(function(waveform) {sign(waveform[[1]][1]) * max(abs(waveform[[1]]))})
  
  return(tibble(gyro_one, gyro_two, gyro_three, 
                extremum_one, extremum_two, extremum_three,
                times))
}


# In: data tibble with gyroscope and touch entries.
#     touch can be "touch" with x/y in headers $one $two. use labeled=TRUE.
#     or touch can be "touch_guess" with no x/y. use labeled=FALSE.
# Ou: if labeled=TRUE, collect the waveform and features with x/y labels.
# Ou: if labeled=FALSE, collect the waveform and features
gyro_from_touch <- function(data, labeled=TRUE) {
  data <- data %>% arrange(time)
  
  ## Use gyro and touch data to mine features
  gyro <- data %>%
    filter(type == "gyroscope")
  if (labeled) {
    times <- data %>%
      filter(type == "touch") %>%
      .$time
  } else {
    times <- data %>% 
      filter(type == "touch_predict") %>%
      .$time
  }
  features <- mine_features(gyro, times)
  
  ## Add labels
  if (labeled) {
    labels <- data %>%
      filter(type=="touch") %>%
      select(x=one, y=two)
    features <- features %>% bind_cols(labels)
  }
  
  return(features)
}


# For testing
foobar <- model %>% trim(0.3, 0.35)
gyro <- foobar %>% filter(type == "gyroscope")
times <- foobar %>% filter(type == "touch") %>% .$time
signal <- gyro %>% select(time=time, data=one)

gyro_from_touch(model %>% trim(0.7, 0.75), labeled=TRUE)
```

Great. It only took 300+ lines of code to get here, from the data presented by the Android app to an actual workable tidy learnable format. I'm going to encapsulate this in one function. I'm also going to use the output of the pressure model as a feature.

```{r}
# In: tibble of pressure data with columns `time` and `p`, 
#     vector of times
# Ou: a feature vector containing the strongest p-value
pull_pressure_features <- function(result_pair, times) {
  pressure_data <- bind_rows(result_pair$testing %>% mutate(stage="testing"),
                             result_pair$training %>% mutate(stage="training")) %>%
    select(time, p, stage) %>%
    arrange(time)
  
  i <- 1
  features <- c()
  for (time in times) {
    index <- which(pressure_data$time > time)[1]
    values <- pressure_data$p[(index-2):(index+5)]
    features[i] <- max(values)
    i <- i + 1
  }
  return(features)
}

build_experiment <- function(tidbits=NULL, path="data/experiment_1.csv") {
  if (is.null(tidbits)) {
    tidbits <- read_csv(path)
  }
  
  data <- clean_tidbits(tidbits)
  
  ## Build dataframes for train and test
  ## 60/40 split
  prop = 0.6
  touch_data <- data %>%
  	filter(type == "touch") %>%
  	arrange(time)
  cutoff <- touch_data$time[as.integer(nrow(touch_data)*prop)]
  train <- data %>%
    filter(time < cutoff) %>%
    filter(type %in% c("pressure", "touch")) %>%
    arrange(time)
  test <- data %>%
    filter(time > cutoff) %>%
    filter(type %in% c("pressure", "touch")) %>%
    arrange(time)
  
  
  ## Use pressure data to build a pressure model,
  result_pair <- train_pressure_model(train, test, width=4)

  ## threshold the pressure model for touch_predict tidbits
  thresholded_testing <- threshold_pressure(result_pair$testing, threshold=0.5) %>%
    bind_rows(test %>% filter(type=="touch"))
  ## and combine the results
  model <- data %>%
    bind_rows(thresholded_testing %>%
                filter(type=="touch_predict") %>%
                select(-p)) %>%
    bind_rows(thresholded_training %>%
                filter(type=="touch_predict") %>%
                select(-p))
  
  exp_data <- gyro_from_touch(model, labeled=TRUE) %>%
    mutate(deflection = pull_pressure_features(result_pair, times))

  
  result <- list()
  result$exp_data <- exp_data
  result$pressure_training <- result_pair$training
  result$pressure_testing <- result_pair$testing
  return(result)
}

```

<TODO>
Wait hold up. I need to modify the function above so that when I mine exp_data, I also get a feature from the pressure data. This will be used to predict the radius of touch input. I'll need a helper function so that I can input a vector of times and the p-model data, and it'll mine features for me. I thought about re-using pull_waveforms, but that pulls from zero-centered data. My p-model goes between 0 and 1, so........ maybe check the three values before and the five values after and get the max. That'll be my feature.

```{r}

```
</TODO>

Alright. Experiment one. Can I discern left/right movement?

```{r}
exp1_data <- build_experiment(path="data/experiment_1.csv") %>%
  .$exp_data %>%
  mutate(side = ifelse(x > 500, "right", "left"))

# The experiment is distributed left/right
exp1_data %>% 
  ggplot(aes(x=x, y=y)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 1:", subtitle="Can we discern left from right?")

# This scatterplot matrix indicates relation with extrema from two and three
ggpairs(exp1_data, 
        columns=c(4:6,8),
        mapping = ggplot2::aes(color=side))


bounds <- exp1_data[4:6] %>% abs() %>% max()

exp1_data %>%
  ggplot(aes(x=extremum_two, y=extremum_three, color=side)) +
  geom_point() +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) 
  ggtitle("Experiment 1 is discernible")
  
# For SEAL application
fig1a <- exp1_data %>% 
  ggplot(aes(x=x, y=y, color=side)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Taps on phone display") +
  xlab("horizontal pixels") +
  ylab("vertical pixels")

fig1a

fig1b <- exp1_data %>%
  ggplot(aes(x=extremum_two, y=extremum_three, color=side)) +
  geom_point() +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  xlab("Rotation on y-axis") +
  ylab("Rotation on z-axis") +
  ggtitle("Latent space")

fig1b

fig1 <- ggarrange(fig1a, fig1b, common.legend=TRUE, legend="bottom") #TODO export for SEAL
```

I can in fact discern left/right movement. Visually, I can already tell that simple decision tree classification would work well for this experiment. I must remember that while experiment 1 is binary classification, the final problem is regression.


Experiment two: Do taps in the center vs. the edge differ in pressure deviation?
 TODO
```{r}
exp2 <- build_experiment(path="data/experiment_2.csv")

exp2_data <- exp2 %>%
  .$exp_data %>%
  mutate(side = ifelse(y > 1800, "lower", "center"))

exp2_pressure_testing <- exp2$pressure_testing

# The experiment is distributed center/lower
exp2_data %>% 
  ggplot(aes(x=x, y=y, color=side)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 2:", subtitle="Can we discern center from lower?")


ggpairs(exp2_data, 
        columns=c(4:6, 9),
        mapping = ggplot2::aes(color=side))


bounds <- exp2_data[4:6] %>% abs() %>% max()

exp2_data %>%
  ggplot(aes(x=extremum_two, y=extremum_three, color=side)) +
  geom_point() +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  ggtitle("Experiment 2 is not discernible with gyro")
```
  
TODO the data shows this experiment is not answerable by gyro data. I will have to see about using pressure height. This will require getting my hands dirty with the functions, so I will do this later.

TODO ya know, this plot demonstrates that usage of a scatterplot isn't really smart here. The human eye isn't so great at discerning patterns in point clouds. If I learned anything from CSE 412, it's that I can aggregate. Perhaps I can use a diverging colorscheme with a visible center (RdYlBu) to indicate confusion between the two classes. Then, in order to indicate a lack of data, I could alter the luminesence or saturation of the color, or I could even re-add the points on top of the aggregation.

# Experiment 3
```{r}
group_by_quantile <- function(vec, num_of_groups=4) {
  breaks <- seq(from=0, to=1, length.out=num_of_groups+1)
  quantiles <- quantile(vec, probs=breaks)
  boundaries <- quantiles[2:length(quantiles)]
  
  class <- vec %>% sapply(function(x) {
    which(x <= boundaries)[1]
  })
  
  return(factor(unname(class)))
}


exp3_data <- build_experiment(path="data/experiment_3.csv") %>%
  .$exp_data %>%
  mutate(quartile=group_by_quantile(x+y))

# The experiment is distributed in an arc
exp3_map <- exp3_data %>% 
  ggplot(aes(x=x, y=y, color=quartile)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 3:", subtitle="A simple regression problem\n(one response variable)")


# If x and y were distributed in a quarter circle, I would compute the angle of each point around the circle to do regression on a single variable
# Since x and y are strongly collinear (r=0.97), I can safely discard one of the responses.

# This scatterplot matrix indicates
exp3_matrix <- ggpairs(exp3_data, 
        columns=c(4:6,8),
        mapping = ggplot2::aes(color=quartile))

bounds <- exp3_data[4:6] %>% abs() %>% max()

exp3_space_all <- exp3_data %>%
  ggplot(aes(x=extremum_one, y=extremum_two, color=quartile)) +
  geom_point(size=2, alpha=0.8) +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  ggtitle("Experiment 3", 
          subtitle="the space seems too cloudy for regression..") +
  xlab("Rotation on x-axis") +
  ylab("Rotation on y-axis")

exp3_space_filter <- exp3_data %>%
  filter(quartile %in% c(1,3)) %>%
  ggplot(aes(x=extremum_one, y=extremum_two, color=quartile)) +
  geom_point(size=2, alpha=0.8) +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  ggtitle("",subtitle="but the results show separation on a medium scale.") +
  xlab("Rotation on x-axis") +
  ylab("Rotation on y-axis")

exp3_space <- ggarrange(exp3_space_all, exp3_space_filter)

exp3_map
exp3_matrix
exp3_space


fig3a <- exp3_map + 
  ggtitle("Taps on phone display", subtitle=NULL) +
  xlab("horizontal pixels") +
  ylab("vertical pixels")
fig3b <-  ggarrange(exp3_space_all + 
                      ggtitle("Latent space", subtitle=NULL),
                    exp3_space_filter + 
                      ggtitle("Latent space", subtitle="first and third quartiles only"),
                    nrow=2,
                    ncol=1,
                    legend=FALSE)
fig3b

fig3 <- ggarrange(fig3a, fig3b, common.legend=TRUE, legend="bottom")
fig3
```

Okay. I haven't finished experiment 2 yet, but experiments 1 and 3 have given me some faith in this project.

Let's try a realistic dataset.

```{r}
exp4 <- build_experiment(path="data/raw 3-28-2021.csv")

exp4_data <- exp4$exp_data %>%
  mutate(col = 1*(x<380) + 2*(380<x&x<700) + 3*(700<x),
         row = 1*(y<1550) + 2*(1550<y&y<1680) + 3*(1680<y&y<1880) + 4*(1880<y)) %>%
  mutate(digit = ifelse(row==4, 0, col+3*row-3))

# The deflection is strangely distributed. 
# There are two modes, visually separable around the median of 0.8.
hist(exp4_data$deflection, breaks=20)
quantile(exp4_data$deflection)
# Coloring exp4_map with group_by_quantile(deflection, num_of_groups=[2 or 4])
# suggests that there's something valuable here, separating the bottom of the
# screen (digits 0 and 8) from the rest.
# To further examine, I will try a continuous color scale.
# It would be nice to find a transform that makes this distribution uniform, but
# simple tricks don't seem to work. Thus, I will try something impractical and 
# check the rank variables.

exp4_data <- exp4_data %>%
  arrange(deflection) %>%
  mutate(deflection_rank=1:nrow(exp4_data)) %>%
  arrange(times)


exp4_map <- exp4_data %>% 
  ggplot(aes(x=x, y=y, color=deflection_rank)) +
  geom_point(size=3, alpha=0.5) +
  scale_color_viridis_c() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 4:", subtitle="A real-world scenario")


exp4_matrix <- ggpairs(exp4_data, 
        columns=c(4:6,8))

exp4_map
exp4_matrix

fig4 <- exp4_map +
  xlab("horizontal pixels") +
  ylab("vertical pixels") +
  ggtitle("Taps on phone display", subtitle="ranked by amount of deflection in pressure sensor")
```

TODO I've got to label this data by row, col, number.
    (c1)  (c3)
(r1)  1  2  3    
      4  5  6
      7  8  9
(r4)     0       

c1-c2-c3 breaks at 380 and 700

r1-r2-r3-r4 breaks at 1550, 1680, 1880

then the digit at row $r < 4$ and col $c$ is $$c+ 3r - 3$$,
and the digit at row 4 is 0.