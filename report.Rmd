---
title: "Big Brother Barometer"
author: "Rishabh Verma"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally) # for ggpairs scatterplot matrix
library(ggpubr) # for multiple ggplots in one figure
library(tidyverse)
library(glmnet) # for lasso/ridge
library(ggthemes)
library(e1071) # for svm
library(fuzzyjoin)  # for mining a pressure feature

## Filepath on my desktop
setwd("E:\\Desktop\\projects\\docs-BBBarometer")

## Filepath on my laptop
#setwd("C:\\Users\\risha\\OneDrive\\Desktop\\docs-BBBarometer")
```

## Introduction

My phone has a gyroscope and a barometer, which respectively measure rotational velocity and air pressure. This paper centers around the phenomenon that when I touch the screen, the barometer registers a spike in air pressure, likely because the force of my finger causes a volume deflection inside the phone. The gyroscope is susceptible to recording tremors in the hand holding the phone, but if the barometer can indicate WHEN touch input occurs, the gyroscope can indicate WHERE on the screen the touch input occurs. This could be applied to create a malicious background process which can read a user's PIN code with access only to barometer and gyroscope sensor data. This report is a work in progress.



```{r include=FALSE}
#tidbits <- read_csv("data/experiment_1.csv")

# TODO just use the line above
tidbits <- read_csv("data/trial.csv")
#tidbits <- read_csv("data/sporadic.csv")
```

## Data Cleaning


```{r}
# Clean tidbits and create tibble `data`
source("cleaning_and_plotting.R")
data <- clean_tidbits(tidbits)
rm(tidbits)
```


## Phase 1: Finding touch occurrence from pressure data

Now to detect touch occurrence. I could do something dumb and threshold the pressure data, or I could do something clever and train a model. Perhaps I keep a buffer of the last four pressure readings, or simply the last four changes in pressure reading. This will be tuple of 4 predictors. To form a training dataset, I need responses; perhaps the four-tuple nearest in time

                 |                              |     
. . . . . x x x x|. . .        . . . . . . . . .|x x x x . .
                 |                              |     
nearest in time before            nearest in time after
   (unviable)                            (viable)

can be categorized as 1, and all other four-tuples (excepting maybe the neighboring ones) can be categorized as 0.

After training a binary classifier (logit distribution), we will then threshold on the logit probability. 

### 1a: Examine pressure data

```{r}
scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=0)
#scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=1, stem=TRUE)
#scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=2, stem=TRUE)
```

It looks like each pressure spike is registered on about two points. Let's use a window of four derivative values, which requires computation on a window of size five. Around each touch point, I will record two subsequent windows as label "one".

### 1b: Label pressure data with touch input

Let's use the first 60\% of the data as training data, and the last 40\% as testing data. With 161 data points in experiment 1, this is 101 data points in training and 69 data points in testing.

```{r}
prop = 0.6  # proportion of training data

## Portion data into train and test
data <- data %>%
  mutate(set = ifelse(time < quantile(time, prop),
                      "training", "testing"))

rm(prop)
```


### 1c: Predict touch input from pressure data

Let's just try a logistic regression. I'm concerned this isn't really the best choice of model because we don't have a close-to-even split and the noise may not be gaussian. This also does not leverage the fact that we are dealing with a time series, but let's just try it out.

```{r include=FALSE}
source("phase_one_functions.R")  # literally just the following functions:

# build_df <- function(pressure_data, width, incidences=2)
#
# For labeling snapshots of pressure data with 1 or 0
# input: a dataframe containing pressure data and touch data
#        width: the size of a single window
#        incidences: the number of windows to record following a touch event
# output: a dataframe where each row is small window of the signal, and is labeled.
#         and a final entry for the label


# train_pressure_model <- function(train, test, width=4, incidences=2)
#
# input: tibbles with the tidbits, and a width parameter
# this function will restructure the tidbits for regression,
#                    train the model,
#                    and return the fitting for train and test.
# the width describes the size of the window passed across the signal
# the windowed signal is used in logistic regression


# threshold_pressure <- function(touch_confidence, threshold=0.4)
#
# Input a dataframe with pressure data and the modeled p-confidence of touch occurrence.
# Include the value at which to threshold.
# Inserts rows of type "touch_predict" labeled by set "training" or "testing"
# Assumes all training data happens before testing data.
```

# TODO this plot can be used to identify points to mark as 1 or as 0

build_df(train, width=5, incidences=4) %>% tibble() %>% mutate(label=factor(label)) %>% mutate(row=1:872) %>% ggplot(aes(x=row, y=X6, color=label, alpha=label)) + geom_point() + scale_alpha_manual(values=c(0.2,0.3), breaks=factor(c(0,1))) + xlim(250,500)

```{r}
# this is just pressure data with a p-column
touch_confidence <- train_pressure_model(data, width=5, incidences=4)

# This threshold does pretty well
threshold <- evaluate_thresholds(data, touch_confidence, sets="training") %>%
  .$threshold_F1

# this includes touch_predict events and pressure-p measurements
thresholded <- threshold_pressure(touch_confidence, threshold=threshold)

# Now put the touch_predicts into the big data tibble
data <- bind_rows(data, thresholded %>% 
                    filter(type == "touch_predict") %>% 
                    select(-p))
rm(thresholded)
```

Here's what the result of phase one looks like. The choice in threshold is explained later.

```{r}
a=0.5
w=0.15

model_signal <- scatterplot_pressure_model(data, touch_confidence,
                           a=a, b=a+w,
                           threshold=threshold,
                           sensor_type="pressure", col="one")

# scatterplot_pressure_model(data, touch_confidence,
#                            a=a, b=a+w,
#                            threshold=threshold,
#                            sensor="gyroscope", col="one")

rm(a, w)
```

In the top figure, the red vertical lines are physical taps. The grey signal is pressure data.

In the bottom figure, the red signal is the output of the "pressureâ†’touch" model. When this signal crosses a threshold, the model predicts a touch, displayed as a blue dashed vertical line.

<!-- Wow! That looks really good. I really hate proceeding forward without objective measures of everything I do. I really bet our model could be improved using lasso regression, and that would require hyperparameter tuning which is unreliable without a measurable error function, but "time to market" is of more importance. -->



### 1d: Evaluate model

#### The metrics

So we've got real touches (red lines) and predicted touches (dashed blue lines). 

* When a red line is accompanied by a dashed blue line, that's a true positive ($TP$).

* When a red line is not accompanied by a dashed blue line, that's a false negative ($FN$).

* When a dashed blue line is not near any red line, that's a false positive ($FP$).

* In this model, there is no meaningful notion of a true negative; there is nothing to count.

So we've got three quantities we can work with, and three meaningful metrics. 

What proportion of recorded events are actually real touches? This is the positive predictive value (PPV), equal to $\dfrac{TP}{TP+FP}$.

What proportion of real touches is the model able to capture? This is the true positive rate (TPR), equal to $\dfrac{TP}{TP+FN}$

Using a signal processing analogy, PPV describes how much of the measurement is noise, and TPR describes how much of the signal the measurement captures. A model that is too sensitive may have excellent TPR, but poor PPV as it keeps falsely triggering without a touch input. A model that is too strict may have excellent PPV, but poor TPR as it misses a lot of the signal.

Let's compare PPV and TPR at a bunch of different thresholds.

<!-- TODO Flow of the writeup must be like: "Present data, build model, find threshold from training data, show model in signal form with this threshold (without further elaboration), THEN talk about thresholds -- show model in performance-summary form, talk about F-statistics" -->
  
#### The graphs

```{r include=FALSE}
performance_by_threshold <- evaluate_thresholds(data, touch_confidence, sets="testing") %>%
  .$performance_by_threshold


threshold_line <- geom_vline(xintercept=threshold, color="#D3C4BC", linetype=2)

performance_by_threshold_long <- performance_by_threshold %>%
  pivot_longer(cols=c(F1, Fhalf, F2), names_to="metric", values_to="F") %>%
  mutate(metric = factor(metric, levels=c("Fhalf", "F1", "F2")))


plot1 <- performance_by_threshold %>%
  pivot_longer(cols = c(PPV, TPR), names_to = "metric") %>%
  ggplot(aes(x=threshold, y=value, color=metric)) +
  geom_line(size=1) +
  geom_point() +
  scale_y_continuous(limits=c(0,1)) +
  scale_x_reverse(limits=function(xlims) { c(1, xlims[2]) }) +
  coord_cartesian(ylim=c(0,1)) +
  scale_color_manual(breaks = c("PPV", "TPR"), 
                     values = c("#619cff", "#44bc2f"),
                     labels = c("Precision (% useful captures)", 
                                "Recall (% touches captured)")) +
  ggtitle("Precision and recall against threshold") +
  threshold_line +
  labs(x="threshold (decreasing)") +
  theme_minimal() +
  theme(legend.title=element_blank()) +
  geom_vline(xintercept=1, color="black", size=1) +
  geom_hline(yintercept=0, color="black", size=1)

num_of_touches <- data %>% filter(set=="testing" & type == "touch") %>% nrow()

plot2 <- ggplot() +
  geom_col(data=performance_by_threshold %>%
             filter(TP>FP) %>%
             pivot_longer(cols=c(TP,FN), names_to="classification", values_to="count"),
           mapping=aes(x=threshold, y=count, fill=classification)) +
  scale_fill_manual(values=c("TP"="#1D8909",    #00BB44
                             "FN"="#CCDACA",    #A0CCA9
                             "FP"="#D01C8B")) + #EE4444  
  geom_col(data=performance_by_threshold,
           mapping=aes(x=threshold, y=-FP),
           fill="#D01C8B") +
  scale_x_reverse(limits=c(1,0)) +
  ggtitle("Classifications by decreasing threshold") +
  threshold_line +
  labs(x="threshold (decreasing)") +
  theme_minimal() +
  ylim(-num_of_touches, num_of_touches)

# TODO annotate plot2 with recall
  
plot2b <- plot2 +
  geom_line(data=performance_by_threshold_long,
            mapping=aes(x=threshold, y=F*(TP+FN), linetype=metric, color=metric, size=metric)) +
  scale_linetype_manual(values=c("F2"=2, "F1"=1, "Fhalf"=3)) +
  scale_color_manual(values=c("F2"="#DB3E00", "F1"="#E7AB0B", "Fhalf"="#82BDFE")) +
  scale_size_manual(values=c("F2"=1, "F1"=1, "Fhalf"=1.3)) +
  ggtitle("Classifications by decreasing threshold",
          subtitle="with three precision-recall metrics")

plot3 <- performance_by_threshold_long %>%
  mutate(metric = factor(metric, levels=c("F2","F1","Fhalf"))) %>%  # order, for facet_grid
  group_by(metric) %>%
  mutate(F=F/max(F)) %>%
  filter(PPV > 0) %>%
  ggplot(aes(x=threshold, y=F, color=F)) +
  geom_point(size=2) +
  geom_line() +
  facet_grid(metric ~ .) + 
  scale_x_reverse(limits=function(xlims) { c(1, xlims[2]) }) +
  scale_y_continuous(limits=c(0,1), minor_breaks=seq(0,1,0.25)) +
  scale_color_continuous(type="viridis") +
  geom_vline(aes(xintercept=threshold),
           colour="#FA272A", linetype=2,
           data=performance_by_threshold_long %>%
             group_by(metric) %>%
             filter(F==max(F)) %>%
             filter(threshold==min(threshold)) %>%  # remove competing thresholds
             mutate(Fscore=metric)) +
  labs(y="F (scaled to 1)",
       x="threshold (decreasing)",
       color="F (scaled to 1)") +
  ggtitle("Three 'optimal' thresholds,", subtitle="according to three F-metrics") +
  theme_bw()

rm(performance_by_threshold_long, threshold_line)
```


> Let's see how threshold affects precision and recall.

From left to right, the threshold lowers and the model becomes more sensitive (Fig 1). As the recall (blue) approaches 1, the precision of positive predictions (red) decreases.
```{r}
plot1
```

> But proportions are too abstract; let's count the actual classifications. 

In Figure 2, the pale green bar represents the positives that need to be captured. From left to right, the threshold lowers and the model is able to capture true positives. As the threshold lowers too far, though, the model captures more and more false positives.

```{r}
plot2
```
 
> I understand "green good, red bad", but how about a measurable accuracy metric?

I thought about using the unit norm or the geometric mean of precision and recall, but I decided on F-score because it's [well-documented](https://en.wikipedia.org/wiki/F-score). I can weight it so that either precision ($F_{0.5}$) or recall ($F_2$) is more important. Let's throw that on the previous plot.

```{r}
plot2b
```

> Which metric is better?

You can clearly see that Fhalf is biased to the left, where precision is indeed higher, but it's difficult to see where each metric reaches its maximum. Let's take the green plot and color-code by each F-statistic. The purple values are hiding in the F-statistic, 



The coloring in Fig 3 is scaled so that the "best" threshold as measured by each metric is similarly bright.

```{r}
plot3
```
```{r save the figures so far, include=FALSE}
# id=6
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig0_model",id,".png",sep=""), width=720, height=480)
# model_signal
# dev.off()
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig1_model",id,".png",sep=""), width=720, height=480)
# plot1
# dev.off()
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig2b_model",id,".png",sep=""), width=720, height=480)
# plot2b
# dev.off()
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig3_model",id,".png",sep=""), width=720, height=480)
# plot3
# dev.off()
```


1. I should try models with and without zero-intercepts.
2. I should try models with and without some accelerometer data.
3. I should double check whether logistic regression even makes sense, or if all linear models are identical.

* No Rishabh, logistic models are not simply linear models on the logarithms of the data

4. I should figure out a metric to compare model performance in testing vs training to avoid overfitting. Maybe peak PPV*TPR in either?
  
* F half works great!

5. Why are these PPV curves so weird?? 

* PPV = TP/(TP+FP). The denominator is the number of positives, and the numerator is the proportion of positives. It's actually pretty close to constant across all thresholds, and not that weird. This suggests that there isn't a ton of variance left to explain.

5a. Why does it suggest there isn't a ton of variance left to explain?

* Suppose there was unexplained variance. A strict model has high PPV. A looser model has proportionally lower PPV because the "less obvious" touches are difficult to distinguish from noise. 

6. Why are these TPR curves so weird??

* TPR = TP/(TP+FN). The denominator is a constant, equal to the number of "touch" datapoints in the training data. Thus, the plot of TPR~threshold is proportional to TP~threshold. The reason TP dips below full is because the concept of thresholding a signal makes no sense when that threshold is near the minimum value of the signal. For example, 

```{r} 
touch_confidence %>% ggplot(aes(x=time, y=p)) + geom_line() + geom_hline(yintercept=0.2, colour="grey", linetype=2) + ggtitle("This threshold is nonsensical")
```  

* It might make sense to only use the threshold where TPR is non-increasing.

7. Once I have a model that's good, is it going to do well on an independent data source?


Well, at least I can typically do better than 80% PPV and 80% TPR.

### 1e: Tune model parameters

We're going to use the F0.5 metric to choose our threshold.

### 1f: Improve model (TODO)


## Phase 2: Mining features for direction of turn

Okay next phase! Given touch input (or a proxy for its occurrence), can I identify a relationship between gyroscope data and the location of the touch input?   



#### Thinking what to mine from gyroscope data

Which features do I mine? 

Perhaps the next 10 datapoints.
Perhaps their slopes.
Perhaps I subsample the next 10 datapoints.
Perhaps the point of greatest absolute value in the neighborhood.
  What is a neighborhood? Is it simply 10 points before and 10 points after? Or maybe I can be clever with it around the zeros. Suppose my pressure data points me to a positive accelerometer peak. I could consider everything between the surrounding zeros as the peak. I could use the width of this peak as a statistical measure.
  
This peak will have n points. How do I mine this peak?????

I really need some sort of book on statistical methods for time series

#### 2a: Mine data surrounding each potential touch event

```{r include=FALSE}
# pull_waveforms <- function(signal, times)
#   Get waveforms (peaks) from a ZERO-CENTERED signal at a set of times


# mine_features <- function(data, touch_confidence, times)
#   Mine features from gyroscope data and pressure data at given times


# build_df_touch <- function(data, touch_confidence, labeled=TRUE)
#   Given a dataset containing sensor and (touch | touch_predict), create a dataframe 
#   to predict touch location from sensor features.
#     if labeled, read from touch data and include x/y
#     if unlabeled, read from touch_predict data (later used to predict x/y)


# xy2digit <- function(df)
#   Input a dataframe with columns x, y
#   Return the dataframe with added columns `col`, `row`, `digit`


# build_experiment <- function(tidbits=NULL, path="data/trial.csv")
#   from filepath or tidbits, predict touch and build a dataframe of touch_predicts 

source("phase_two_functions.R")
```



### 2b: Look at data

These first plots give me a broad exposition into the data

```{r}
# plot_responses <- events %>%
#   filter(isTouch) %>%
#   ggpairs(mapping=aes(color=digit),
#           columns=c("x","y","r","d","m","theta"),
#           upper=list(continuous = "points", combo = "facethist", discrete = "facetbar", na =
#                        "na"),
#           lower=list(continuous = "points", combo = "facethist", discrete = "facetbar", na =
#                        "na"),
#           diag=list(continuous = "blankDiag", discrete = "blankDiag", na = "blankDiag")) +
#   scale_color_brewer(type="qual",
#                      palette="Set3") +
#   theme_bw()
```

```{r}
exp1 <- build_experiment(path="data/trial.csv")
events <- exp1$events

a=0.8
w=0.05
b=a+w



a=0.83
b=0.838

#a=0.823
#b=0.827

# a=0.807
# b=0.812

scatterplot(exp1$data, "gyroscope", "two", start=a, end=b, stem=TRUE) +
  geom_point(aes(x=two_right_time_predict, y=two_right_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=two_left_time_predict, y=two_left_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=two_right_time_touch, y=two_right_touch),
             data=events, color="red") +
  geom_point(aes(x=two_left_time_touch, y=two_left_touch),
             data=events, color="red")
```




### 2c: x+y ~ .  (naive)

```{r include=FALSE}
# first we build a model using   predict features on known touch data in training phase
# then we evaluate a model using predict features on unknown predict data in testing phase
events.training <- events %>%
  filter(set=="training") %>%
  filter(isTouch) %>%
  mutate(one_width_touch = one_right_time_touch - one_left_time_touch,
         two_width_touch = two_right_time_touch - two_left_time_touch,
         three_width_touch = three_right_time_touch - three_left_time_touch) %>%
  select(one_left_touch, one_right_touch, one_width_touch,
         two_left_touch, two_right_touch, two_width_touch,
         three_left_touch, three_right_touch, three_width_touch,
         pressure=pressure_touch,
         x, y, r, m, d, theta)
events.training.response <- events.training %>%
  select(x, y, r, m, d, theta)
events.training.data <- events.training %>% 
  select(-x, -y, -r, -m, -d, -theta) %>%
  as.matrix()
colnames(events.training.data) = paste("x", 1:ncol(events.training.data), sep="_")


events.testing <- events %>%
  filter(set=="testing") %>%
  filter(isPredict & isTouch) %>%  # we need Touch so we can compute actual residuals
  mutate(one_width_predict = one_right_time_predict - one_left_time_predict,
         two_width_predict = two_right_time_predict - two_left_time_predict,
         three_width_predict = three_right_time_predict - three_left_time_predict) %>%
  select(one_left_predict, one_right_predict, one_width_predict,
         two_left_predict, two_right_predict, two_width_predict,
         three_left_predict, three_right_predict, three_width_predict,
         pressure=pressure_predict,
         x, y, r, m, d, theta)
events.testing.response <- events.testing %>%
  select(x, y, r, m, d, theta)
events.testing.data <- events.testing %>% 
  select(-x, -y, -r, -m, -d, -theta) %>%
  as.matrix()
colnames(events.testing.data) = paste("x", 1:ncol(events.testing.data), sep="_")


```

Now I need a function that takes in all the svm parameters and does what I want.
What do I want?

```{r}
# x.* are data matrices, and y.* are response vectors, passed into svm()
# metric.function takes arguments `actual` and `fitted` and returns a 
#   value that increases as actual is closer to fitted (e.g. cor, AIC, accuracy)
train_svm <- function(x.training, y.training, x.testing, y.testing, 
                      metric.function=stats::cor, ...) {
  model.touch <- svm(x.training, y.training, ...)
  fit <- predict(model.touch, x.testing)
  
  result.training <- tibble(residuals = model.touch$residuals,
                            fitted = model.touch$fitted,
                            actual = model.touch$fitted + model.touch$residuals)
  
  result.testing <- tibble(fitted = fit,
                           actual = y.testing,
                           residuals = fitted - actual)
  
  result = list(result.testing=result.testing,
                result.training=result.training,
                metric=metric.function(result.testing$actual, result.testing$fitted),
                model=model.touch)
  return(result)
}

x.prediction <- train_svm(events.training.data, events.training.response$x, 
                          events.testing.data, events.testing.response$x)
y.prediction <- train_svm(events.training.data, events.training.response$y, 
                          events.testing.data, events.testing.response$y)
r.prediction <- train_svm(events.training.data, events.training.response$r, 
                          events.testing.data, events.testing.response$r)
theta.prediction <- train_svm(events.training.data, events.training.response$theta, 
                          events.testing.data, events.testing.response$theta)

prediction = theta.prediction
plot(prediction$result.testing$fitted, prediction$result.testing$actual)
```

How we doing in training and in testing?

```{r}
xy.result.training <- bind_cols(
  x.prediction$result.training %>%
    mutate(id=1:nrow(x.prediction$result.training)) %>%
    select(fitted, actual, id) %>%
    pivot_longer(cols=c("fitted", "actual"), names_to="data", values_to="x"),
  y.prediction$result.training %>%
    mutate(id=1:nrow(y.prediction$result.training)) %>%
    select(fitted, actual, id) %>%
    pivot_longer(cols=c("fitted", "actual"), names_to="data", values_to="y")) %>%
  select(x, y, data=data...2, id=id...1) %>%
  xy2digit() %>%
  pivot_wider(id_cols="id", names_from="data", values_from=c("x","y","col","row","digit")) %>%
  select(-id)
```

```{r}
xy.result.testing <- bind_cols(
  x.prediction$result.testing %>%
    mutate(id=1:nrow(x.prediction$result.testing)) %>%
    select(fitted, actual, id) %>%
    pivot_longer(cols=c("fitted", "actual"), names_to="data", values_to="x"),
  y.prediction$result.testing %>%
    mutate(id=1:nrow(y.prediction$result.testing)) %>%
    select(fitted, actual, id) %>%
    pivot_longer(cols=c("fitted", "actual"), names_to="data", values_to="y")) %>%
  select(x, y, data=data...2, id=id...1) %>%
  xy2digit() %>%
  pivot_wider(id_cols="id", names_from="data", values_from=c("x","y","col","row","digit")) %>%
  select(-id)

xy.result.testing %>%
  select(col_fitted, col_actual) %>%
  table()

xy.result.testing %>%
  select(row_fitted, row_actual) %>%
  table()

xy.result.testing %>%
  select(digit_fitted, digit_actual) %>%
  table()
```

### 2d: x+y ~ (r|d|m) + theta


### 2e: ((1)) + theta ~ .

### 2f: Combine 2c and 2e


## Phase 3: Code-reading

#### Phase 3a: Planning

I need a clean phone with a clean screen protector.  
I need an app with no unnecessary text, a non-distracting color scheme, and a non-distracting image.  



#### Phase 3b: Prepping Data

Along the way, we will be storing everything in .rda files or something

First, collect a large amount of random data.  
1a. Clean the tidbits into data  ( IMPORTANT: subtract median pressure at this step )
                                ( also remove sensor data before/after type -1, type -2)
1b. Label as 60% as training, 40% as testing [^1]
1c. Build train_df and test_df separately with build_df(data, width=5, incidences=4)  (cue Jason Bramburger saying you just gotta set some numbers and run with it)
1d. Build touches from data %>% filter(type=="touch")
1e. Store data, train_df, test_df, touches as incrementing .rda files for later [^2]
1f. Combine each thing into one large thing and save that as four big .rda's

[^1] the threshold is part of the overall model, and its selection requires evaluation on an independent set of labeled data to select
[^2] e.g. data_1, data_2, ..., data_n; train_df_1, train_df_2, ..., train_df_n; ...

Now, we build a model.
2a. Read train_df and test_df, then build a pressure model manually.
2b. Save model.
2c. Build touch_confidence manually. Save touch_confidence.
2d. Conduct threshold_trials, save threshold, make figures ?
2e. Threshold the pressure, create data_complete, save as .rda
2f. Build_df_touch(data_complete, touch_confidence, labeled=TRUE)

Now, we repeat as much as we need to for Jenny's data
3a. Clean tidbits into data
3b. Build train_df
3c. Use saved model to build touch_confidence
3d. Threshold on the previously chosen threshold
3e. Put the events in data_complete_jenny
3f. build_df_touch(data_complete, touch_confidence, labeled=FALSE)

We'll read the numbers WITHOUT using the labels, and use the labels only for evaluation.



Then, collect a large amount of Jenny's number data.
4a. Again, clean the tidbits into data
4b. Label as testing this time
4c. Build test_df




If it all goes well, we can do a random thing.



#### Phase 3c: Reading Jenny's Number

Let's try reading Jenny's number

```{r}
# phase_1_data
# phase_1_train_df
# phase_1_test_df
# phase_1_touches
load("data/phase_1_clean/phase_1_data.Rda")



# Now, we build a model.
# 2a. Read train_df and test_df, then build a pressure model manually.
train_df <- phase_1_train_df %>% 
  mutate(a=X1, b=X3+X4, pressure = pressure - median(pressure)) %>% select(a,b,pressure,label)
test_df <- phase_1_test_df %>% 
  mutate(a=X1, b=X3+X4, pressure = pressure - median(pressure)) %>% select(a,b,pressure,label)
touch_model <- glm(label ~ ., 
                   data=train_df,
                   family = "binomial")

# 2b. Save model.
save(touch_model, file="data/phase_1_clean/touch_model.Rda")

# 2c. Build touch_confidence manually. Save touch_confidence.
pressure_and_touch <- phase_1_data %>% filter(type %in% c("pressure", "touch"))
w=5
pressure_model_data <- build_df(pressure_and_touch, width=w, incidences=4)
training_data <- pressure_model_data %>%
  mutate(a=X1,b=X3+X4) %>%
  select(a, b, pressure)
output <- predict(touch_model,
                  newdata=training_data)
fitted.values <- 1/(1+exp(-1*(output))) %>%
  lead(3, default=0)

pressure <- pressure_and_touch %>%
  filter(type == "pressure")

touch_confidence <- pressure_and_touch %>%
  filter(type == "pressure") %>%
  slice(w:(w+length(output)-1)) %>%
  bind_cols(p=fitted.values)
#touch_confidence <- train_pressure_model(phase_1_data, width=5, incidences=4)


# let's double check visually
# a = 0.0, b=0.00002
data = phase_1_data
scatterplot_pressure_model(data=phase_1_data, touch_confidence=touch_confidence,
                           a=0.00005, b=0.00009,
                           threshold=0.3)

# 2d. Conduct threshold_trials, save threshold, make figures ?
threshold_trials <- evaluate_thresholds(data=phase_1_data, touch_confidence, sets=c("testing"))
threshold <- mean(c(threshold_trials$threshold_F1, threshold_trials$threshold_Fhalf))
thresholded <- threshold_pressure(touch_confidence, threshold)

# 2e. Threshold the pressure, create data_complete, save as .rda
touch_predicts <- thresholded %>% filter(type == "touch_predict") %>% select(-p)
data_complete <- bind_rows(phase_1_data, touch_predicts)
save(data_complete, file="data_complete.Rda")

# 2f. Build_df_touch(data_complete, touch_confidence, labeled=TRUE)
build_df_touch(data_complete, touch_confidence, labeled=TRUE)
```


Hmm. Issues.
I'm used to receiving a train consisting of a SINGLE experiment, then building train_df, fitting, predicting, and binding the results back to train.
I can't do that here.
But maybe I can get away with using the pre-built model and reshaping the data for model fitting.

