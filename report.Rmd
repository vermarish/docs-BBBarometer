---
title: "Big Brother Barometer"
author: "Rishabh Verma"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(GGally) # for ggpairs scatterplot matrix
library(ggpubr) # for multiple ggplots in one figure
library(tidyverse)
library(glmnet) # for lasso/ridge
library(ggthemes)
library(e1071) # for svm
library(fuzzyjoin)  # for mining a pressure feature

## Filepath on my desktop
# setwd("E:\\Desktop\\docs-BBBarometer")

## Filepath on my laptop
setwd("C:\\Users\\risha\\OneDrive\\Desktop\\docs-BBBarometer")
```

## Introduction

My phone has a gyroscope and a barometer, which respectively measure rotational velocity and air pressure. This paper centers around the phenomenon that when I touch the screen, the barometer registers a spike in air pressure, likely because the force of my finger causes a volume deflection inside the phone. The gyroscope is susceptible to recording tremors in the hand holding the phone, but if the barometer can indicate WHEN touch input occurs, the gyroscope can indicate WHERE on the screen the touch input occurs. This could be applied to create a malicious background process which can read a user's PIN code with access only to barometer and gyroscope sensor data. This report is a work in progress.



```{r include=FALSE}
#tidbits <- read_csv("data/experiment_1.csv")

# TODO just use the line above
tidbits <- read_csv("data/trial.csv")
#tidbits <- read_csv("data/sporadic.csv")
```

## Data Cleaning

----
# TODO temporary experiment to look at average pressures
```{r}
# tidbits <- read_csv("data/pressure variance.csv")
# data <- clean_tidbits(tidbits)
# pressure_data <- data %>% filter(type=="pressure")
# time <- pressure_data %>% pull(time)
# a=0.1
# b=0.4
# c=0.5
# d=0.86
# 
# plot1 <- pressure_data %>% 
#   ggplot(aes(x=time, y=one)) + 
#   geom_point() + geom_line() + 
#   geom_vline(xintercept=quantile(time,c(a,b,c,d)), color="red") +
#   ggtitle("Pressure signal")
# 
# first_half = pressure_data %>% filter(time > quantile(time,a) & time < quantile(time,b))
# second_half = pressure_data %>% filter(time > quantile(time,c) & time < quantile(time,d))
# foo <- data.frame(pressure=first_half$one, variance="low")
# bar <- data.frame(pressure=second_half$one, variance="high")
# foobar <- bind_rows(foo, bar) %>% mutate(variance=factor(variance)) %>% tibble()
# 
# means <- foobar %>% 
#   group_by(variance) %>%
#   summarize(mean=mean(pressure))
# 
# # First a boxplot
# plot2 <- foobar %>%
#   ggplot(aes(y=pressure, x=variance)) +
#   geom_boxplot() +
#   geom_point(aes(y=mean, x=variance), color="red", data=means) +
#   ggtitle("Medians of pressure signals")
# 
# # Then a histogram
# plot3 <- foobar %>%
#   ggplot(aes(x=pressure)) +
#   geom_histogram() +
#   facet_grid(vars(variance)) +
#   ggtitle("Distributions of pressure signals")
# 
# plot1
# plot2
# plot3
```
It looks like the median is pretty resistant to change. I could use height above the median as a factor...
but wait
in linear regression with an intercept, subtracting the median value from a signal doesn't change the fit at all, does it? 
Fit one: y ~ pressure + beta0 $y = \beta_0 + \beta_1p$
Denote the optimum parameters (optimum given by minimizing $RSS$) as:
 * $\beta_0$: $\hat{\beta_0}$
 * $\beta_1$: $\hat{\beta_1}$
 
Fit two: y ~ (pressure-median) + beta $y = \beta_0 + \beta_1(p-\text{med}(p))$
Consider the following parameters
 * $\beta_0$: $\hat{\beta_0}+\beta_1\text{med}(p)$
 * $\beta_1$: $\hat{\beta_1}$
 
This will lead to the same fitting of $\hat{y}$, and so this will lead to the same $RSS$. 

So LOL I don't need to do a linear transformation of the input variables. Hey, shouldn't that kind of thing be a theorem or a corollary? A linear transformation of predictors does not affect the fit of a linear model.
----

```{r}
# Clean tidbits and create tibble `data`
source("cleaning_and_plotting.R")
data <- clean_tidbits(tidbits)
rm(tidbits)
```


## Phase 1: Finding touch occurrence from pressure data

Now to detect touch occurrence. I could do something dumb and threshold the pressure data, or I could do something clever and train a model. Perhaps I keep a buffer of the last four pressure readings, or simply the last four changes in pressure reading. This will be tuple of 4 predictors. To form a training dataset, I need responses; perhaps the four-tuple nearest in time

                 |                              |     
. . . . . x x x x|. . .        . . . . . . . . .|x x x x . .
                 |                              |     
nearest in time before            nearest in time after
   (unviable)                            (viable)

can be categorized as 1, and all other four-tuples (excepting maybe the neighboring ones) can be categorized as 0.

After training a binary classifier (logit distribution), we will then threshold on the logit probability. 

### 1a: Examine pressure data

```{r}
scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=0)
#scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=1, stem=TRUE)
#scatterplot(data, "pressure", "one", 0.7, 0.8, derivs=2, stem=TRUE)
```

It looks like each pressure spike is registered on about two points. Let's use a window of four derivative values, which requires computation on a window of size five. Around each touch point, I will record two subsequent windows as label "one".

### 1b: Label pressure data with touch input

Let's use the first 60\% of the data as training data, and the last 40\% as testing data. With 161 data points in experiment 1, this is 101 data points in training and 69 data points in testing.

```{r}
prop = 0.6  # proportion of training data

## Portion data into train and test
data <- data %>%
  mutate(set = ifelse(time < quantile(time, prop),
                      "training", "testing"))

rm(prop)
```


### 1c: Predict touch input from pressure data

Let's just try a logistic regression. I'm concerned this isn't really the best choice of model because we don't have a close-to-even split and the noise may not be gaussian. This also does not leverage the fact that we are dealing with a time series, but let's just try it out.

```{r include=FALSE}
source("phase_one_functions.R")  # literally just the following functions:

# build_df <- function(pressure_data, width, incidences=2)
#
# For labeling snapshots of pressure data with 1 or 0
# input: a dataframe containing pressure data and touch data
#        width: the size of a single window
#        incidences: the number of windows to record following a touch event
# output: a dataframe where each row is small window of the signal, and is labeled.
#         and a final entry for the label


# train_pressure_model <- function(train, test, width=4, incidences=2)
#
# input: tibbles with the tidbits, and a width parameter
# this function will restructure the tidbits for regression,
#                    train the model,
#                    and return the fitting for train and test.
# the width describes the size of the window passed across the signal
# the windowed signal is used in logistic regression


# threshold_pressure <- function(touch_confidence, threshold=0.4)
#
# Input a dataframe with pressure data and the modeled p-confidence of touch occurrence.
# Include the value at which to threshold.
# Inserts rows of type "touch_predict" labeled by set "training" or "testing"
# Assumes all training data happens before testing data.
```

# TODO this plot can be used to identify points to mark as 1 or as 0

build_df(train, width=5, incidences=4) %>% tibble() %>% mutate(label=factor(label)) %>% mutate(row=1:872) %>% ggplot(aes(x=row, y=X6, color=label, alpha=label)) + geom_point() + scale_alpha_manual(values=c(0.2,0.3), breaks=factor(c(0,1))) + xlim(250,500)

```{r}
# this is just pressure data with a p-column
touch_confidence <- train_pressure_model(data, width=5, incidences=4)

# This threshold does pretty well
threshold <- evaluate_thresholds(data, touch_confidence, sets="training") %>%
  .$threshold_F1

# this includes touch_predict events and pressure-p measurements
thresholded <- threshold_pressure(touch_confidence, threshold=threshold)

# Now put the touch_predicts into the big data tibble
data <- bind_rows(data, thresholded %>% 
                    filter(type == "touch_predict") %>% 
                    select(-p))
rm(thresholded)
```

Here's what the result of phase one looks like. The choice in threshold is explained later.

```{r}
a=0.5
w=0.1

model_signal <- scatterplot_pressure_model(data, touch_confidence,
                           a=a, b=a+w,
                           threshold=threshold,
                           sensor_type="pressure", col="one")

# scatterplot_pressure_model(data, touch_confidence,
#                            a=a, b=a+w,
#                            threshold=threshold,
#                            sensor="gyroscope", col="one")

rm(a, w)
```

In the top figure, the red vertical lines are physical taps. The grey signal is pressure data.

In the bottom figure, the red signal is the output of the "pressureâ†’touch" model. When this signal crosses a threshold, the model predicts a touch, displayed as a blue dashed vertical line.

<!-- Wow! That looks really good. I really hate proceeding forward without objective measures of everything I do. I really bet our model could be improved using lasso regression, and that would require hyperparameter tuning which is unreliable without a measurable error function, but "time to market" is of more importance. -->



### 1d: Evaluate model

#### The metrics

So we've got real touches (red lines) and predicted touches (dashed blue lines). 

* When a red line is accompanied by a dashed blue line, that's a true positive ($TP$).

* When a red line is not accompanied by a dashed blue line, that's a false negative ($FN$).

* When a dashed blue line is not near any red line, that's a false positive ($FP$).

* In this model, there is no meaningful notion of a true negative; there is nothing to count.

So we've got three quantities we can work with, and three meaningful metrics. 

What proportion of recorded events are actually real touches? This is the positive predictive value (PPV), equal to $\dfrac{TP}{TP+FP}$.

What proportion of real touches is the model able to capture? This is the true positive rate (TPR), equal to $\dfrac{TP}{TP+FN}$

Using a signal processing analogy, PPV describes how much of the measurement is noise, and TPR describes how much of the signal the measurement captures. A model that is too sensitive may have excellent TPR, but poor PPV as it keeps falsely triggering without a touch input. A model that is too strict may have excellent PPV, but poor TPR as it misses a lot of the signal.

Let's compare PPV and TPR at a bunch of different thresholds.

<!-- TODO Flow of the writeup must be like: "Present data, build model, find threshold from training data, show model in signal form with this threshold (without further elaboration), THEN talk about thresholds -- show model in performance-summary form, talk about F-statistics" -->
  
#### The graphs

```{r include=FALSE}
performance_by_threshold <- evaluate_thresholds(data, touch_confidence, sets="testing") %>%
  .$performance_by_threshold


threshold_line <- geom_vline(xintercept=threshold, color="#D3C4BC", linetype=2)

performance_by_threshold_long <- performance_by_threshold %>%
  pivot_longer(cols=c(F1, Fhalf, F2), names_to="metric", values_to="F") %>%
  mutate(metric = factor(metric, levels=c("Fhalf", "F1", "F2")))


plot1 <- performance_by_threshold %>%
  pivot_longer(cols = c(PPV, TPR), names_to = "metric") %>%
  ggplot(aes(x=threshold, y=value, color=metric)) +
  geom_line(size=1) +
  geom_point() +
  scale_y_continuous(limits=c(0,1)) +
  scale_x_reverse(limits=function(xlims) { c(1, xlims[2]) }) +
  coord_cartesian(ylim=c(0,1)) +
  scale_color_manual(breaks = c("PPV", "TPR"), 
                     values = c("#619cff", "#44bc2f"),
                     labels = c("Precision (% useful captures)", 
                                "Recall (% touches captured)")) +
  ggtitle("Precision and recall against threshold") +
  threshold_line +
  labs(x="threshold (decreasing)") +
  theme_minimal() +
  theme(legend.title=element_blank()) +
  geom_vline(xintercept=1, color="black", size=1) +
  geom_hline(yintercept=0, color="black", size=1)

num_of_touches <- data %>% filter(set=="testing" & type == "touch") %>% nrow()

plot2 <- ggplot() +
  geom_col(data=performance_by_threshold %>%
             filter(TP>FP) %>%
             pivot_longer(cols=c(TP,FN), names_to="classification", values_to="count"),
           mapping=aes(x=threshold, y=count, fill=classification)) +
  scale_fill_manual(values=c("TP"="#1D8909",    #00BB44
                             "FN"="#CCDACA",    #A0CCA9
                             "FP"="#D01C8B")) + #EE4444  
  geom_col(data=performance_by_threshold,
           mapping=aes(x=threshold, y=-FP),
           fill="#D01C8B") +
  scale_x_reverse(limits=c(1,0)) +
  ggtitle("Classifications by decreasing threshold") +
  threshold_line +
  labs(x="threshold (decreasing)") +
  theme_minimal() +
  ylim(-num_of_touches, num_of_touches)

# TODO annotate plot2 with recall
  
plot2b <- plot2 +
  geom_line(data=performance_by_threshold_long,
            mapping=aes(x=threshold, y=F*(TP+FN), linetype=metric, color=metric, size=metric)) +
  scale_linetype_manual(values=c("F2"=2, "F1"=1, "Fhalf"=3)) +
  scale_color_manual(values=c("F2"="#DB3E00", "F1"="#E7AB0B", "Fhalf"="#82BDFE")) +
  scale_size_manual(values=c("F2"=1, "F1"=1, "Fhalf"=1.3)) +
  ggtitle("Classifications by decreasing threshold",
          subtitle="with three precision-recall metrics")

plot3 <- performance_by_threshold_long %>%
  mutate(metric = factor(metric, levels=c("F2","F1","Fhalf"))) %>%  # order, for facet_grid
  group_by(metric) %>%
  mutate(F=F/max(F)) %>%
  filter(PPV > 0) %>%
  ggplot(aes(x=threshold, y=F, color=F)) +
  geom_point(size=2) +
  geom_line() +
  facet_grid(metric ~ .) + 
  scale_x_reverse(limits=function(xlims) { c(1, xlims[2]) }) +
  scale_y_continuous(limits=c(0,1), minor_breaks=seq(0,1,0.25)) +
  scale_color_continuous(type="viridis") +
  geom_vline(aes(xintercept=threshold),
           colour="#FA272A", linetype=2,
           data=performance_by_threshold_long %>%
             group_by(metric) %>%
             filter(F==max(F)) %>%
             filter(threshold==min(threshold)) %>%  # remove competing thresholds
             mutate(Fscore=metric)) +
  labs(y="F (scaled to 1)",
       x="threshold (decreasing)",
       color="F (scaled to 1)") +
  ggtitle("Three 'optimal' thresholds,", subtitle="according to three F-metrics") +
  theme_bw()

rm(performance_by_threshold_long, threshold_line)
```


> Let's see how threshold affects precision and recall.

From left to right, the threshold lowers and the model becomes more sensitive (Fig 1). As the recall (blue) approaches 1, the precision of positive predictions (red) decreases.
```{r}
plot1
```

> But proportions are too abstract; let's count the actual classifications. 

In Figure 2, the pale green bar represents the positives that need to be captured. From left to right, the threshold lowers and the model is able to capture true positives. As the threshold lowers too far, though, the model captures more and more false positives.

```{r}
plot2
```
 
> I understand "green good, red bad", but how about a measurable accuracy metric?

I thought about using the unit norm or the geometric mean of precision and recall, but I decided on F-score because it's [well-documented](https://en.wikipedia.org/wiki/F-score). I can weight it so that either precision ($F_{0.5}$) or recall ($F_2$) is more important. Let's throw that on the previous plot.

```{r}
plot2b
```

> Which metric is better?

You can clearly see that Fhalf is biased to the left, where precision is indeed higher, but it's difficult to see where each metric reaches its maximum. Let's take the green plot and color-code by each F-statistic. The purple values are hiding in the F-statistic, 



The coloring in Fig 3 is scaled so that the "best" threshold as measured by each metric is similarly bright.

```{r}
plot3
```
```{r save the figures so far, include=FALSE}
# id=6
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig0_model",id,".png",sep=""), width=720, height=480)
# model_signal
# dev.off()
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig1_model",id,".png",sep=""), width=720, height=480)
# plot1
# dev.off()
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig2b_model",id,".png",sep=""), width=720, height=480)
# plot2b
# dev.off()
# 
# while(dev.cur() > 1) dev.off()
# png(paste("fig/fig3_model",id,".png",sep=""), width=720, height=480)
# plot3
# dev.off()
```


1. I should try models with and without zero-intercepts.
2. I should try models with and without some accelerometer data.
3. I should double check whether logistic regression even makes sense, or if all linear models are identical.

* No Rishabh, logistic models are not simply linear models on the logarithms of the data

4. I should figure out a metric to compare model performance in testing vs training to avoid overfitting. Maybe peak PPV*TPR in either?
  
* F half works great!

5. Why are these PPV curves so weird?? 

* PPV = TP/(TP+FP). The denominator is the number of positives, and the numerator is the proportion of positives. It's actually pretty close to constant across all thresholds, and not that weird. This suggests that there isn't a ton of variance left to explain.

5a. Why does it suggest there isn't a ton of variance left to explain?

* Suppose there was unexplained variance. A strict model has high PPV. A looser model has proportionally lower PPV because the "less obvious" touches are difficult to distinguish from noise. 

6. Why are these TPR curves so weird??

* TPR = TP/(TP+FN). The denominator is a constant, equal to the number of "touch" datapoints in the training data. Thus, the plot of TPR~threshold is proportional to TP~threshold. The reason TP dips below full is because the concept of thresholding a signal makes no sense when that threshold is near the minimum value of the signal. For example, 

```{r} 
touch_confidence %>% ggplot(aes(x=time, y=p)) + geom_line() + geom_hline(yintercept=0.2, colour="grey", linetype=2) + ggtitle("This threshold is nonsensical")
```  

* It might make sense to only use the threshold where TPR is non-increasing.

7. Once I have a model that's good, is it going to do well on an independent data source?


Well, at least I can typically do better than 80% PPV and 80% TPR.

### 1e: Tune model parameters

We're going to use the F0.5 metric to choose our threshold.

### 1f: Improve model (TODO)


## Phase 2: Mining features for direction of turn

Okay next phase! Given touch input (or a proxy for its occurrence), can I identify a relationship between gyroscope data and the location of the touch input?   



#### Thinking what to mine from gyroscope data

Which features do I mine? 

Perhaps the next 10 datapoints.
Perhaps their slopes.
Perhaps I subsample the next 10 datapoints.
Perhaps the point of greatest absolute value in the neighborhood.
  What is a neighborhood? Is it simply 10 points before and 10 points after? Or maybe I can be clever with it around the zeros. Suppose my pressure data points me to a positive accelerometer peak. I could consider everything between the surrounding zeros as the peak. I could use the width of this peak as a statistical measure.
  
This peak will have n points. How do I mine this peak?????

I really need some sort of book on statistical methods for time series

#### 2a: Mine data surrounding each potential touch event

```{r include=FALSE}
# pull_waveforms <- function(signal, times)
#   Get waveforms (peaks) from a ZERO-CENTERED signal at a set of times


# mine_features <- function(data, touch_confidence, times)
#   Mine features from gyroscope data and pressure data at given times


# build_df_touch <- function(data, touch_confidence, labeled=TRUE)
#   Given a dataset containing sensor and (touch | touch_predict), create a dataframe 
#   to predict touch location from sensor features.
#     if labeled, read from touch data and include x/y
#     if unlabeled, read from touch_predict data (later used to predict x/y)


# xy2digit <- function(df)
#   Input a dataframe with columns x, y
#   Return the dataframe with added columns `col`, `row`, `digit`


# build_experiment <- function(tidbits=NULL, path="data/trial.csv")
#   from filepath or tidbits, predict touch and build a dataframe of touch_predicts 

source("phase_two_functions.R")
```

UH OH. Last time I did this, I didn't really pay attention to mining good gyro data. We'll have to actually think about this now. 

First, let's look at some accelerometer things.
```{r}
exp1 <- build_experiment(path="data/trial.csv")
events <- exp1$events

# run this chunk once
i = 1
gyro <- exp1$data %>% filter(type=="gyroscope")
touch <- exp1$data %>% filter(type=="touch")
period <- gyro$time %>% diff() %>% median()
radius_l <- period*30
radius_r <- period*90
```

```{r}
# run this chunk iteratively
i = i + 1

t = touch$time[i]
a = t - radius_l
b = t + radius_r

plot_gyro_1 <- scatterplot(exp1$data, "gyroscope", "one", start=0, end=1) +
  xlim(a, b) +
  ylim(-0.46, 0.46) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        title=element_blank()) +
  geom_point(aes(x=one_right_time_predict, y=one_right_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=one_left_time_predict, y=one_left_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=one_right_time_touch, y=one_right_touch),
             data=events, color="red") +
  geom_point(aes(x=one_left_time_touch, y=one_left_touch),
             data=events, color="red")

plot_gyro_2 <- scatterplot(exp1$data, "gyroscope", "two", start=0, end=1) +
  xlim(a, b) +
  ylim(-0.46, 0.46) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        title=element_blank()) +
  geom_point(aes(x=two_right_time_predict, y=two_right_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=two_left_time_predict, y=two_left_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=two_right_time_touch, y=two_right_touch),
             data=events, color="red") +
  geom_point(aes(x=two_left_time_touch, y=two_left_touch),
             data=events, color="red")

plot_gyro_3 <- scatterplot(exp1$data, "gyroscope", "three", start=0, end=1) +
  xlim(a, b) +
  ylim(-0.26, 0.26) +
  theme(axis.title.x=element_blank(),
        axis.title.y=element_blank(),
        title=element_blank()) +
  geom_point(aes(x=three_right_time_predict, y=three_right_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=three_left_time_predict, y=three_left_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=three_right_time_touch, y=three_right_touch),
             data=events, color="red") +
  geom_point(aes(x=three_left_time_touch, y=three_left_touch),
             data=events, color="red")

fig1 <- ggarrange(plot_gyro_1, plot_gyro_2, plot_gyro_3,
          nrow=1, legend=FALSE)
fig1
```

Maybe a sine basis (DFT) ?
Or a wavelet basis (wavelets)
or maybe human-interpretable features
* polarity of steepest change
* peak after, peak before
* (half-height width)(height of each segment)
* area under each segment

i=7
  The highest difference typically accompanies touch input, but may not always be prominent.
i=12, i=15
  What if the gyroscope deviance isn't a climb or a fall but a peak or a valley?



TODO cite https://wires-onlinelibrary-wiley-com.offcampus.lib.washington.edu/doi/full/10.1002/wics.1392

I quite like the idea of
1. Use Perceptually Important Points (PIP)s to reduce the dimension of the signal
2. Find the largest diff in PIP close to our touch event
3. Record the adjacent PIPs as our features

But the shape of a 

You know what, let's keep it simple

1. Compute a 4-diff and find the largest


### 2b: Look at data

These first plots give me a broad exposition into the data

```{r}
plot_responses <- events %>%
  filter(isTouch) %>%
  ggpairs(mapping=aes(color=digit),
          columns=c("x","y","r","d","m","theta"),
          upper=list(continuous = "points", combo = "facethist", discrete = "facetbar", na =
                       "na"),
          lower=list(continuous = "points", combo = "facethist", discrete = "facetbar", na =
                       "na"),
          diag=list(continuous = "blankDiag", discrete = "blankDiag", na = "blankDiag")) +
  scale_color_brewer(type="qual",
                     palette="Set3") +
  theme_bw()

training_data <- events %>%
  filter(isTouch) %>%
  select(one_left_touch, one_right_touch, one_width_touch,
         two_left_touch, two_right_touch, two_width_touch,
         three_left_touch, three_right_touch, three_width_touch,
         x, y, r, m, d, theta)

model.1 <- lm(x ~ . - y - r - m - d - theta,
              data=training_data)

training_data.2 <- training_data %>% 
  mutate(one = one_right_touch - one_left_touch, 
         two = two_right_touch - two_left_touch, 
         three = three_right_touch - three_left_touch) %>%
  select(one, two, three, x)

ggpairs(training_data.2)



model.2 <- lm(x ~ .,
              data=training_data.2)

events %>%
  ggplot() +
  geom_histogram(aes(colour=factor(col), x=two_right_touch-two_left_touch))


# plot_xy <- events %>%
#   filter(isTouch) %>%
#   ggpairs(mapping=aes(color=as.factor(col)),
#           columns=c("x", "y", 
#                     "extremum_one_predict", 
#                     "extremum_two_predict", 
#                     "extremum_three_predict", 
#                     "extremum_pressure_predict", 
#                     "extremum_p_model_predict"),
#           upper=list(continuous = "points", combo = "facethist", discrete = "facetbar", na =
#                        "na"),
#           lower=list(continuous = "points", combo = "facethist", discrete = "facetbar", na =
#                        "na"),
#           diag=list(continuous = "blankDiag", discrete = "blankDiag", na = "blankDiag")) +
#   #scale_color_brewer(type="qual",
#                      #palette="Set3") +
#   theme_bw()
# 
# plot_r_m_d_theta <- events %>%
#   filter(isTouch) %>%
#   ggpairs(mapping=aes(color=as.factor(row)),
#           columns=c("r", "m", "d", "theta", 
#                     "extremum_one_predict", 
#                     "extremum_two_predict", 
#                     "extremum_three_predict", 
#                     "extremum_pressure_predict", 
#                     "extremum_p_model_predict"),
#           upper=list(continuous = "points", combo = "facethist", discrete = "facetbar", na =
#                        "na"),
#           lower=list(continuous = "blank", combo = "facethist", discrete = "facetbar", na =
#                        "na"),
#           diag=list(continuous = "blankDiag", discrete = "blankDiag", na = "blankDiag")) +
#   theme_bw()
# 
# plot2b_extrema_comparison <- events %>%
#   filter(isTouch) %>%
#   select(extremum_one_touch, extremum_two_touch, extremum_three_touch,
#          extremum_one_predict, extremum_two_predict, extremum_three_predict) %>%
#   ggpairs()

#plot_responses
#plot_xy
#plot_r_m_d_theta
#plot2b_extrema_comparison

```

```{r}
exp1 <- build_experiment(path="data/trial.csv")
events <- exp1$events

a=0.8
w=0.05
b=a+w



a=0.83
b=0.838

#a=0.823
#b=0.827

# a=0.807
# b=0.812

scatterplot(exp1$data, "gyroscope", "two", start=a, end=b, stem=TRUE) +
  geom_point(aes(x=two_right_time_predict, y=two_right_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=two_left_time_predict, y=two_left_predict),
             data=events, color="blue", shape=2, size=2) +
  geom_point(aes(x=two_right_time_touch, y=two_right_touch),
             data=events, color="red") +
  geom_point(aes(x=two_left_time_touch, y=two_left_touch),
             data=events, color="red")
```



These next plots include some key results
```{r}
plot2b_1 <- events %>%
  filter(isTouch) %>%
  ggplot(aes(x=extremum_two_touch,
             y=d,
             color=as.factor(col))) +
  geom_point()
plot2b_1

plot2b_2 <- events %>%
  filter(isTouch) %>%
  ggplot(aes(x=extremum_two_touch,
             y=x,
             color=as.factor(col))) +
  geom_point()
plot2b_2
```

### 2c: x+y ~ .  (naive)

```{r include=FALSE}
# first we build a model using   predict features on known touch data in training phase
# then we evaluate a model using predict features on unknown predict data in testing phase
events.training <- events %>%
  filter(set=="training") %>%
  filter(isTouch) %>%
  mutate(one_width_touch = one_right_time_touch - one_left_time_touch,
         two_width_touch = two_right_time_touch - two_left_time_touch,
         three_width_touch = three_right_time_touch - three_left_time_touch) %>%
  select(one_left_touch, one_right_touch, one_width_touch,
         two_left_touch, two_right_touch, two_width_touch,
         three_left_touch, three_right_touch, three_width_touch,
         x, y, r, m, d, theta)
events.training.response <- events.training %>%
  select(x, y, r, m, d, theta)
events.training.data <- events.training %>% 
  select(-x, -y, -r, -m, -d, -theta) %>%
  as.matrix()
colnames(events.training.data) = paste("x", 1:ncol(events.training.data), sep="_")


events.testing <- events %>%
  filter(set=="testing") %>%
  filter(isPredict & isTouch) %>%  # we need Touch so we can compute actual residuals
  mutate(one_width_predict = one_right_time_predict - one_left_time_predict,
         two_width_predict = two_right_time_predict - two_left_time_predict,
         three_width_predict = three_right_time_predict - three_left_time_predict) %>%
  select(one_left_predict, one_right_predict, one_width_predict,
         two_left_predict, two_right_predict, two_width_predict,
         three_left_predict, three_right_predict, three_width_predict,
         x, y, r, m, d, theta)
events.testing.response <- events.testing %>%
  select(x, y, r, m, d, theta)
events.testing.data <- events.testing %>% 
  select(-x, -y, -r, -m, -d, -theta) %>%
  as.matrix()
colnames(events.testing.data) = paste("x", 1:ncol(events.testing.data), sep="_")


```

Now I need a function that takes in all the svm parameters and does what I want.
What do I want?

```{r}
# x.* are data matrices, and y.* are response vectors, passed into svm()
# metric.function takes arguments `actual` and `fitted` and returns a 
#   value that increases as actual is closer to fitted (e.g. cor, AIC, accuracy)
train_svm <- function(x.training, y.training, x.testing, y.testing, 
                      metric.function=stats::cor, ...) {
  model.touch <- svm(x.training, y.training, ...)
  fit <- predict(model.touch, x.testing)
  
  result.training <- tibble(residuals = model.touch$residuals,
                            fitted = model.touch$fitted,
                            actual = model.touch$fitted + model.touch$residuals)
  
  result.testing <- tibble(fitted = fit,
                           actual = y.testing,
                           residuals = fitted - actual)
  
  result = list(result.testing=result.testing,
                result.training=result.training,
                metric=metric.function(result.testing$actual, result.testing$fitted),
                model=model.touch)
  return(result)
}

x.prediction <- train_svm(events.training.data, events.training.response$x, 
                          events.testing.data, events.testing.response$x)
y.prediction <- train_svm(events.training.data, events.training.response$y, 
                          events.testing.data, events.testing.response$y)
r.prediction <- train_svm(events.training.data, events.training.response$r, 
                          events.testing.data, events.testing.response$r)
theta.prediction <- train_svm(events.training.data, events.training.response$theta, 
                          events.testing.data, events.testing.response$theta)

prediction = theta.prediction
plot(prediction$result.testing$fitted, prediction$result.testing$actual)
```

```{r}
model.x.training <- svm(x=events.training.data,
                        y=events.training.response$x)

result.x.training <- tibble(residuals = model.x.training$residuals,
                            fitted = model.x.training$fitted,
                            actual = model.x.training$fitted + model.x.training$residuals)

#plot(result.x.training$fitted, result.x.training$residuals)
#plot(result.x.training$fitted, result.x.training$actual)


fit <- predict(model.x.training, events.testing.data)
result.x.testing <- tibble(fitted = fit,
                           actual = events.testing.response$x,
                           residuals = fitted - actual)
#plot(result.x.testing$fitted, result.x.testing$residuals)
#plot(result.x.testing$fitted, result.x.testing$actual)
```

### 2d: x+y ~ (r|d|m) + theta


### 2e: ((1)) + theta ~ .

### 2f: Combine 2c and 2e


EVERYTHING BELOW IS DEPRECATED
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|



Alright. Experiment one. Can I discern left/right movement?

```{r}
exp1_data <- build_experiment(path="data/experiment_1.csv") %>%
  .$exp_data %>%
  mutate(side = ifelse(x > 500, "right", "left"))

# The experiment is distributed left/right
exp1_data %>% 
  ggplot(aes(x=x, y=y)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 1:", subtitle="Can we discern left from right?")

# This scatterplot matrix indicates relation with extrema from two and three
ggpairs(exp1_data, 
        columns=c(4:6,8),
        mapping = ggplot2::aes(color=side))


bounds <- exp1_data[4:6] %>% abs() %>% max()

exp1_data %>%
  ggplot(aes(x=extremum_two, y=extremum_three, color=side)) +
  geom_point() +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) 
  ggtitle("Experiment 1 is discernible")
  
# For SEAL application
fig1a <- exp1_data %>% 
  ggplot(aes(x=x, y=y, color=side)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Taps on phone display") +
  xlab("horizontal pixels") +
  ylab("vertical pixels")

fig1a

fig1b <- exp1_data %>%
  ggplot(aes(x=extremum_two, y=extremum_three, color=side)) +
  geom_point() +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  xlab("Rotation on y-axis") +
  ylab("Rotation on z-axis") +
  ggtitle("Latent space")

fig1b

fig1 <- ggarrange(fig1a, fig1b, common.legend=TRUE, legend="bottom") #TODO export for SEAL
```

I can in fact discern left/right movement. Visually, I can already tell that simple decision tree classification would work well for this experiment. I must remember that while experiment 1 is binary classification, the final problem is regression.


Experiment two: Do taps in the center vs. the edge differ in pressure deviation?
 TODO
```{r}
exp2 <- build_experiment(path="data/experiment_2.csv")

exp2_data <- exp2 %>%
  .$exp_data %>%
  mutate(side = ifelse(y > 1800, "lower", "center"))

exp2_pressure_testing <- exp2$pressure_testing

# The experiment is distributed center/lower
exp2_data %>% 
  ggplot(aes(x=x, y=y, color=side)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 2:", subtitle="Can we discern center from lower?")


ggpairs(exp2_data, 
        columns=c(4:6, 9),
        mapping = ggplot2::aes(color=side))


bounds <- exp2_data[4:6] %>% abs() %>% max()

exp2_data %>%
  ggplot(aes(x=extremum_two, y=extremum_three, color=side)) +
  geom_point() +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  ggtitle("Experiment 2 is not discernible with gyro")
```
  
TODO the data shows this experiment is not answerable by gyro data. I will have to see about using pressure height. This will require getting my hands dirty with the functions, so I will do this later.

TODO ya know, this plot demonstrates that usage of a scatterplot isn't really smart here. The human eye isn't so great at discerning patterns in point clouds. If I learned anything from CSE 412, it's that I can aggregate. Perhaps I can use a diverging colorscheme with a visible center (RdYlBu) to indicate confusion between the two classes. Then, in order to indicate a lack of data, I could alter the luminesence or saturation of the color, or I could even re-add the points on top of the aggregation.

# Experiment 3
```{r}
group_by_quantile <- function(vec, num_of_groups=4) {
  breaks <- seq(from=0, to=1, length.out=num_of_groups+1)
  quantiles <- quantile(vec, probs=breaks)
  boundaries <- quantiles[2:length(quantiles)]
  
  class <- vec %>% sapply(function(x) {
    which(x <= boundaries)[1]
  })
  
  return(factor(unname(class)))
}


exp3_data <- build_experiment(path="data/experiment_3.csv") %>%
  .$exp_data %>%
  mutate(quartile=group_by_quantile(x+y))

# The experiment is distributed in an arc
exp3_map <- exp3_data %>% 
  ggplot(aes(x=x, y=y, color=quartile)) +
  geom_point() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 3:", subtitle="A simple regression problem\n(one response variable)")


# If x and y were distributed in a quarter circle, I would compute the angle of each point around the circle to do regression on a single variable
# Since x and y are strongly collinear (r=0.97), I can safely discard one of the responses.

# This scatterplot matrix indicates
exp3_matrix <- ggpairs(exp3_data, 
        columns=c(4:6,8),
        mapping = ggplot2::aes(color=quartile))

bounds <- exp3_data[4:6] %>% abs() %>% max()

exp3_space_all <- exp3_data %>%
  ggplot(aes(x=extremum_one, y=extremum_two, color=quartile)) +
  geom_point(size=2, alpha=0.8) +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  ggtitle("Experiment 3", 
          subtitle="the space seems too cloudy for regression..") +
  xlab("Rotation on x-axis") +
  ylab("Rotation on y-axis")

exp3_space_filter <- exp3_data %>%
  filter(quartile %in% c(1,3)) %>%
  ggplot(aes(x=extremum_one, y=extremum_two, color=quartile)) +
  geom_point(size=2, alpha=0.8) +
  geom_hline(yintercept=0, color="#aaaaaa") + 
  geom_vline(xintercept=0, color="#aaaaaa") + 
  xlim(-bounds, bounds) +
  ylim(-bounds, bounds) +
  ggtitle("",subtitle="but the results show separation on a medium scale.") +
  xlab("Rotation on x-axis") +
  ylab("Rotation on y-axis")

exp3_space <- ggarrange(exp3_space_all, exp3_space_filter)

exp3_map
exp3_matrix
exp3_space


fig3a <- exp3_map + 
  ggtitle("Taps on phone display", subtitle=NULL) +
  xlab("horizontal pixels") +
  ylab("vertical pixels")
fig3b <-  ggarrange(exp3_space_all + 
                      ggtitle("Latent space", subtitle=NULL),
                    exp3_space_filter + 
                      ggtitle("Latent space", subtitle="first and third quartiles only"),
                    nrow=2,
                    ncol=1,
                    legend=FALSE)
fig3b

fig3 <- ggarrange(fig3a, fig3b, common.legend=TRUE, legend="bottom")
fig3
```

Okay. I haven't finished experiment 2 yet, but experiments 1 and 3 have given me some faith in this project.

Let's try a realistic dataset.

```{r}
exp4 <- build_experiment(path="data/raw 3-28-2021.csv")

exp4_data <- exp4$exp_data %>%
  mutate(col = 1*(x<380) + 2*(380<x&x<700) + 3*(700<x),
         row = 1*(y<1550) + 2*(1550<y&y<1680) + 3*(1680<y&y<1880) + 4*(1880<y)) %>%
  mutate(digit = ifelse(row==4, 0, col+3*row-3))

# The deflection is strangely distributed. 
# There are two modes, visually separable around the median of 0.8.
hist(exp4_data$deflection, breaks=20)
quantile(exp4_data$deflection)
# Coloring exp4_map with group_by_quantile(deflection, num_of_groups=[2 or 4])
# suggests that there's something valuable here, separating the bottom of the
# screen (digits 0 and 8) from the rest.
# To further examine, I will try a continuous color scale.
# It would be nice to find a transform that makes this distribution uniform, but
# simple tricks don't seem to work. Thus, I will try something impractical and 
# check the rank variables.

exp4_data <- exp4_data %>%
  arrange(deflection) %>%
  mutate(deflection_rank=1:nrow(exp4_data)) %>%
  arrange(times)


exp4_map <- exp4_data %>% 
  ggplot(aes(x=x, y=y, color=deflection_rank)) +
  geom_point(size=3, alpha=0.5) +
  scale_color_viridis_c() +
  theme(aspect.ratio=2400/1080) +      ##formatting
  scale_y_reverse(limits=c(2400,0)) +  ##
  scale_x_continuous(limits=c(0,1080), ##
                     position="top") + ##
  ggtitle("Experiment 4:", subtitle="A real-world scenario")


exp4_matrix <- ggpairs(exp4_data, 
        columns=c(4:6,8))

exp4_map
exp4_matrix

fig4 <- exp4_map +
  xlab("horizontal pixels") +
  ylab("vertical pixels") +
  ggtitle("Taps on phone display", subtitle="ranked by amount of deflection in pressure sensor")
```

TODO I've got to label this data by row, col, number.
    (c1)  (c3)
(r1)  1  2  3    
      4  5  6
      7  8  9
(r4)     0       

c1-c2-c3 breaks at 380 and 700

r1-r2-r3-r4 breaks at 1550, 1680, 1880

then the digit at row $r < 4$ and col $c$ is $$c+ 3r - 3$$,
and the digit at row 4 is 0.


-->